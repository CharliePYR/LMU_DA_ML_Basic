{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd650d8-44a6-438a-a8b5-ed54a236b332",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow and Keras\n",
    "\n",
    "We don't have to code up back propagation for every possible function or neural network architecture that we want to fit. There are lots of libraries targeted towards machine learning that make this task easy and computationally efficient. One of the most popular libraries is [TensorFlow](https://www.tensorflow.org/). It was developed by Google Brain and is now open source under the Apache License 2.0.\n",
    "\n",
    "(Other popular choices in 2022 are [PyTorch](https://pytorch.org/) and [JAX](https://jax.readthedocs.io/))\n",
    "\n",
    "The workflow consists of building a computational graph where \"operations\" act on \"tensors\" that can be automatically differentiated. Starting from tensorflow version 2 the operations are by default executed \"eagerly\" such that one can work with tensors in a similar way as with numpy arrays and typically does not have to worry about building the graph.\n",
    "\n",
    "The TensorFlow website contains a much more [detailed introduction](https://www.tensorflow.org/guide/low_level_intro) if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1fdbb-753f-4a42-91e9-8e5c5962464b",
   "metadata": {},
   "source": [
    "## Numpy-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad00c2-4ef9-46f0-9cb7-5c7af15f1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f50297-382e-4989-9af6-857d05877b86",
   "metadata": {},
   "source": [
    "Tensors can be created via `tf.constant` from python lists or numpy arrays. Similar to numpy arrays, they have a `shape` and a `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bac640-ad37-4774-b467-ae3f9af5701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([1, 2, 3], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9c3c0-bd7a-4593-b17f-4e78abe48e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(np.array([1, 2, 3]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09eed9-3bed-417c-aeec-16d2415c3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([[1, 2], [3, 4], [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc02bb-12c1-496b-adc5-8019b8724f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([[[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12]],\n",
    "             [[13, 14, 15, 16],\n",
    "              [17, 18, 19, 20],\n",
    "              [21, 22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d758d1-1f9c-4f21-9f15-dec96876218d",
   "metadata": {},
   "source": [
    "There are also convenience functions, e.g. to create equidistant or random values and all sorts of mathematical functions that represent operations on tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666ec74-1e3e-423d-a0c8-3288dbd46b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.uniform((10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09c472-71c9-4b84-94e6-e803a17467e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.linspace(0., 2.*np.pi, 10)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bcb16-e6ff-4ab8-a7e1-4c4e726cbb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26206402-d556-4af7-b510-3977a10de4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sin(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94e871-3c7e-481d-8497-d5ba2bd12e66",
   "metadata": {},
   "source": [
    "Tensors can be plotted like numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629c631-e670-49b7-8d22-db692fd301ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, tf.sin(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db5c65-ea1d-4c05-8e71-9c468a5f856d",
   "metadata": {},
   "source": [
    "Or explicitely converted via `.numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca2738-6599-4189-a46e-06120cf9bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80210262-11bf-4a39-9153-406144ece7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sin(t).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa53f9-6272-4af4-984a-f84fd860c9dd",
   "metadata": {},
   "source": [
    "## Auto differentiation\n",
    "The real power comes from tracing operations that allows automatic backpropagation to calculate gradients. This can be done using `tf.GradientTape`. By default the gradients w.r.t. tensors (constants) are not recorded, but only for `tf.Variable`. A `tf.Variable` represents a mutable state - this makes sense, since in many cases we want to modify the values on which we calculate gradients (e.g. training a neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1cad6c-04a2-437d-90b8-357a8743f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.Variable(tf.linspace(0., 2.*np.pi, 100))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cf7f1-b6ea-488c-9ac0-921b0e1c7872",
   "metadata": {},
   "source": [
    "We can now calculate the derivative of the `sin` function w.r.t. `t` using `tf.GradientTape` in a context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504ffe0-c054-43f9-be11-88e8fd164e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    f = tf.sin(t)\n",
    "df = tape.gradient(f, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a21b40-8089-4dad-8391-701f7a3d8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for plotting tf.Variable one always has to explicitely convert via .numpy()\n",
    "# (not nescessary for Tensors/tf.constant)\n",
    "plt.plot(t.numpy(), f, label=\"sin(t)\")\n",
    "plt.plot(t.numpy(), df, label=\"sin'(t)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01046fbf-6013-447b-aa37-b50268bfe404",
   "metadata": {},
   "source": [
    "To calculate gradients w.r.t. Tensors (`tf.constant`) instead of `tf.Variable`, use `tape.watch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7d78c-f471-4239-9370-700412761c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_const = tf.linspace(0., 2.*np.pi, 100)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(t_const)\n",
    "    f = tf.sin(t_const)\n",
    "plt.plot(t_const, f, label=\"sin(t)\")\n",
    "plt.plot(t_const, tape.gradient(f, t_const), label=\"sin'(t)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8176aa3-5d98-4a05-9446-a6281d0fa2f0",
   "metadata": {},
   "source": [
    "The computation of the gradient can also be recorded and we can calculate the gradient of the gradient to get the second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562c2d-2b64-4397-a8fd-74579547bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape0:\n",
    "    with tf.GradientTape() as tape1:\n",
    "        f = tf.sin(t)\n",
    "    df = tape1.gradient(f, t)\n",
    "ddf = tape0.gradient(df, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8399c44-befa-4036-9163-172f7120e883",
   "metadata": {},
   "source": [
    "The two gradient tapes are nescessary since tensorflow by default only allows one gradient to be calculated from a tape. If recording gradients themselves to the tape is intended one has to pass `persistent=True` - so the following works as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc0045-ef30-4ab3-b5bf-d4031a8edaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    f = tf.sin(t)\n",
    "    # this is inside the with block, so the gradient itself will also be recorded to the gradient tape\n",
    "    df = tape.gradient(f, t)\n",
    "# now we can calculate the gradient of the gradient\n",
    "ddf_alternative = tape.gradient(df, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f842e1-b52e-4dfa-8625-6398d5156197",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t.numpy(), f.numpy(), label=\"sin(t)\")\n",
    "plt.plot(t.numpy(), df.numpy(), label=\"sin'(t)\")\n",
    "plt.plot(t.numpy(), ddf.numpy(), label=\"sin''(t)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1bb2e-fd80-4d97-9210-3183a1710df8",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "The most convenient way to use TensorFlow with neural networks is through [Keras](http://keras.io). It provides a high-level interface that is somewhat a compromise between very high-level abstractions like scikit-learn and the complete control of every detail you get when directly using the low-level APIs of libraries like TensorFlow. There is a separate [Keras Documentation](https://keras.io), as well as [Guides](https://www.tensorflow.org/guide/keras), [Tutorials](https://www.tensorflow.org/tutorials/keras), and the [Keras section on the TensorFlow API Documentation](https://www.tensorflow.org/api_docs/python/tf/keras).\n",
    "\n",
    "Keras is the recommended/default way to work with neural networks in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a2a3b-5f4b-4048-9e57-04585977347b",
   "metadata": {},
   "source": [
    "## Build a model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661001ff-be05-4f4c-af2d-47ffdc41ba17",
   "metadata": {},
   "source": [
    "As a quick example, let's again build a model to classify the \"Moons\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09462-9786-46ca-b6ac-0bbc08bdd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cd821-44e9-492b-8664-9f682d98fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_moons(n_samples=10000, noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2ba5a-51a7-4824-8a9f-f8b8052fe15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*x[y==0].T, label=\"y=0\", alpha=0.1)\n",
    "plt.scatter(*x[y==1].T, label=\"y=1\", alpha=0.1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2096a-b0e6-4ea9-b319-f4e8f0611e7f",
   "metadata": {},
   "source": [
    "There are 3 ways to use Keras - via the Sequential API, the Functional API or via creating layers and models by subclassing. Lets start with [`Sequential`](https://keras.io/guides/sequential_model/). This is convenient for all models where we just have one input and one output Tensor with stacked Layers in between. Here we use the `Dense` layer - which is precisely the fully connected NN layer that applies the $\\sigma(W\\mathbf{x} + \\mathbf{b})$ operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3063e86-5744-4e36-9021-a65530f48a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Hidden layer with 2 inputs, 16 outputs\n",
    "    Dense(16, activation=\"relu\", input_shape=(2,)),\n",
    "    # Output layer with 16 inputs (determined automatically) and 1 output\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac48d50-37cb-4da7-9b2c-d56bb914b6ad",
   "metadata": {},
   "source": [
    "How much parameters will our model have? The answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f9a6c-63c3-42ee-9eb7-ad594c2af62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb94e1-d9c6-49b6-9ed1-2eaa18cc090d",
   "metadata": {},
   "source": [
    "We can also access the underlying Tensors if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808782f-7af4-429d-a87e-fc0050765713",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af39fab-1c46-4881-8781-183a232f393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3eb813-f9b8-4cb4-a7c9-398d3bd91c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6e6fa-66aa-406d-94f7-40cea8d3150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ede79f-39b3-4d80-a742-8b3622bfae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb45b0-ab62-460e-9e1e-af53a61925fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1734b-9aea-47b9-a9c5-b6ef563b25ff",
   "metadata": {},
   "source": [
    "Both models and layers are callables, so you can feed them tensors to get transformed outputs. This can be very useful to experiment and understand what transformations are done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80277c9f-75fe-467b-9893-3ec1b17d808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca2eaa-fd43-4a3e-8b09-5e7b9cf9ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4483795-67ff-4229-9e67-992255420c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Dense(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e40daf-0bb8-4717-b8ac-ea6546c89c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e0dbf-9e6e-4cf5-896f-b713b8d9901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54cd9b-6e41-4f88-ae58-f00c69993755",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(inputs, layer.weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce748a-f899-421b-8772-5ff79ae1fff1",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc35cf0-e194-4f34-b53d-0ae27230e3ae",
   "metadata": {},
   "source": [
    "Before we can run the training, we have to \"compile\" the model. This will configure the loss function and optimization Algorithm. You cat pass each loss from [`keras.losses`](https://keras.io/losses) and each optimizer from [`keras.optimizers`](https://keras.io/optimizers) also as a string with the name if you want to use it with default parameters. Here we want to use the \"Adam\" optimizer with an adjusted initial learning rate, so we pass it directly.\n",
    "\n",
    "We could also pass some metrics that we want to monitor during training (in addition to the Loss value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea3aa7-207a-4424-9437-49a4d7d61b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab858a1-f833-4b05-b57f-613594d1a16f",
   "metadata": {},
   "source": [
    "The API for fitting looks similar to scikit-learn, but has additional options. In fact there also is a [scikit-learn API  wrapper](https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn) for Keras if you need that in some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ed832-a974-451a-888e-42dd547e0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x, y, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3a407-e289-4cfc-b145-76a26eb45276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e0b77-4030-4168-bd70-985cea0053a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d203ee5-dbbf-426e-8542-ad0d772e3cef",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "The model can be run using `model.predict` or simply calling it like a function on an input. The main difference is that `model.predict` supports several parameters (like `batch_size`) and returns a numpy array whereas calling the model like a function returns a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49dbe8-faa8-4301-8710-0a692577a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16091279-b479-44f8-8143-90a35cc97083",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.meshgrid(\n",
    "    np.arange(x[:,0].min(), x[:,0].max(), 0.1),\n",
    "    np.arange(x[:,1].min(), x[:,1].max(), 0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932726be-f85c-4e7c-96ac-903e410fca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.stack([grid[0].ravel(), grid[1].ravel()], axis=1)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae2670-79a2-4205-92d5-a1e2d53b3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deeb07c-dda6-4202-bdbb-93fd18c3375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0886b2c-260e-4d5d-aa88-486fb703aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model(xy).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae2e14-5102-404b-803f-e89eea9ef5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(grid[0], grid[1], scores.reshape(grid[0].shape), cmap=\"Spectral_r\")\n",
    "plt.colorbar(label=\"NN output\")\n",
    "opts = dict(alpha=0.1, marker=\".\", edgecolors=\"black\")\n",
    "plt.scatter(x[y==0][:,0], x[y==0][:,1], color=\"blue\", **opts)\n",
    "plt.scatter(x[y==1][:,0], x[y==1][:,1], color=\"red\", **opts)\n",
    "plt.xlim(grid[0].min(), grid[0].max())\n",
    "plt.ylim(grid[1].min(), grid[1].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac01771",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p>\n",
    "        <b>Question 1</b>: Do we need a hidden layer? What would happen if we dropped it?\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Question 2</b>: What would happen if we used a linear activation function but kept the hidden layer?\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Question 3</b>: What would happen if we shrank the size of the hidden layer?\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c129a-50fa-4e9d-8d68-bb4543e725d7",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "https://keras.io/guides/functional_api\n",
    "\n",
    "The [functional API](https://keras.io/guides/functional_api/) allows building an arbitrary computation graph composed of keras layers in an abstract way (just specifying input/output shapes, but no data yet). Each layer can be called as a function on an input Tensor and return an output Tensor. One can then finally build a model by passing the input and output Tensors to the `Model` constructor. This is especially useful when we want to organize the processing into different inputs and different outputs or if you want to build computation graphs that have branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e383c4a-3c3f-467e-8540-124219eb425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8349775-f6d5-4527-b9d6-5fa38b86f082",
   "metadata": {},
   "source": [
    "All models start with one or more `Input` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351703d5-e64f-4e00-b209-d716b6783100",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(2,))\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c26a4-2b8b-4e54-a293-3d8271243e66",
   "metadata": {},
   "source": [
    "New nodes in the computation graph are then added by calling Layers with their inputs as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02cce0-cc12-4fc5-9209-12c73cc0eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = layers.Dense(16, activation=\"relu\")(inp)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4efe0c-36de-488f-ac96-47e6054e652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = layers.Dense(1, activation=\"sigmoid\")(hidden)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d498a0-5718-45c2-a07b-f2525322fdce",
   "metadata": {},
   "source": [
    "To create a model, specify the inputs and outputs in the `Model` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84ab03-0989-4717-a739-6cc4f40784c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.Model(inputs=[inp], outputs=[out])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe49bc-9148-432f-8bf9-f375811caa91",
   "metadata": {},
   "source": [
    "Example for a model with 2 inputs and 2 outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ac9ff-dc9f-4c9c-ba35-a87621d7ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inp1 = layers.Input(shape=(3,))\n",
    "    inp2 = layers.Input(shape=(5,))\n",
    "    hidden1 = layers.Dense(16, activation=\"relu\")(inp1)\n",
    "    hidden2 = layers.Dense(16, activation=\"relu\")(inp2)\n",
    "    hidden3 = layers.Concatenate()([hidden1, hidden2])\n",
    "    out1 = layers.Dense(1, activation=\"sigmoid\")(hidden3)\n",
    "    out2 = layers.Dense(1, activation=\"linear\")(hidden3)\n",
    "    return tf.keras.Model(inputs=[inp1, inp2], outputs=[out1, out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948f28b-003b-485b-8b1a-791ce37c074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614524b-ef5f-4ce3-8da1-91a789e67727",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model([np.random.rand(10, 3), np.random.rand(10, 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d881bd0-415c-41ba-a4e3-c3a05a334d5c",
   "metadata": {},
   "source": [
    "To train such models, one would then specify multiple loss functions (one for each output) in `.compile` - the total loss will then be the sum of all losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700bc18-403a-4e65-9752-9b6ec67b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model.compile(loss=[\"binary_crossentropy\", \"mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394eeffd-5912-458e-9684-7ec2a0b4afa1",
   "metadata": {},
   "source": [
    "You can visualize the Graph to see if everything is connected correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984ab2f-4395-42c2-bfca-f93e7ec06dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(multi_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037150c-31f6-4683-ac39-e76ac9731c0c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise:</b> Create a model for the classification of the \"Moons\" dataset that outputs both the hidden layer state and the classification output.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3148c-d269-46fe-a903-367e24802761",
   "metadata": {},
   "source": [
    "## Subclass API\n",
    "https://keras.io/guides/making_new_layers_and_models_via_subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416cfe4-691d-47b5-8685-ce4176b93f95",
   "metadata": {},
   "source": [
    "For maximum flexibility you can also inherit from `tf.keras.models.Model` or `tf.keras.layers.Layer` and implement your own forward pass. This is very similar to how [PyTorch models are commonly built](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html).\n",
    "\n",
    "Both for models and for layers the minimum amount of methods that you have to implement are `__init__`, where you typically define parameters and any state and then the forward pass in `call`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b842f5d-959a-4db3-81c9-f2fc25c334ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseReluLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        # call the base class constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # initialize weights\n",
    "        self.kernel = tf.Variable(tf.random.uniform((n_inputs, n_outputs)))\n",
    "        self.biases = tf.Variable(tf.zeros(n_outputs))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(tf.matmul(inputs, self.kernel) + self.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f43631-102e-403e-8c81-ab6b60e4040b",
   "metadata": {},
   "source": [
    "Custom layers can be arbitrarily combined with existing layers e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7dfd2-4ae9-46d9-8312-ed1cfa263393",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_model = tf.keras.models.Sequential([\n",
    "        MyDenseReluLayer(2, 5),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175e6b7-ac78-4067-be80-944ab3fb321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be85bb5-5313-4c06-8b92-68f70a9195f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa4291-5fa0-4def-b6aa-bd903dd72ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dade865-2b01-48ea-b46c-cf11f425171e",
   "metadata": {},
   "source": [
    "Models can also be used as layers for new models and you can use existing layers as members of custom layers etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd191d-f7eb-4e78-a644-141bc7282d0d",
   "metadata": {},
   "source": [
    "## Visualize hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a923909-585f-4954-b57a-7219b9db7712",
   "metadata": {},
   "source": [
    "For models created with the Sequential or functional API it is easy to create new models that evaluate only part of the computation graph.\n",
    "Let's use this to visualize the hidden layers of our first neural network in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a887c2-9d51-4e63-b916-9c95a97e5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a8366-e58c-4d88-ac1c-28db5d92d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd654abb-364d-4c5f-8d30-69027bd4e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3077c0-a8a5-4584-9c04-b746974fbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_output = tf.keras.Model(inputs=[model.input], outputs=[model.layers[0].output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef373aa-71f1-487d-8020-9c7af5492339",
   "metadata": {},
   "source": [
    "Let's feed it with a regular grid again for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24d239-e9d7-4150-b8f6-4d6517c81441",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.1\n",
    "grid = np.meshgrid(\n",
    "    np.arange(x[:,0].min(), x[:,0].max()+step, step),\n",
    "    np.arange(x[:,1].min(), x[:,1].max()+step, step)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a90678-1b72-476d-bcdc-4a5aff6056fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = np.stack([grid[0].ravel(), grid[1].ravel()], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fbad8-e5c8-4878-9cec-8b4460390b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_out = hidden_output(xp).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55778b3-3b76-4e0d-a91f-f73406bcc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    axs.ravel()[i].contourf(grid[0], grid[1], hl_out[:,i].reshape(grid[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e266f-c9b1-4e23-b0a1-4dae917fae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[1].weights[0]\n",
    "bias = model.layers[1].weights[1]\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24517ac0-2baf-437d-8503-981bbcce2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=16, ncols=2, figsize=(2 * 2, 2 * 16))\n",
    "total = np.zeros_like(hl_out[:, 0])\n",
    "for i in range(16):\n",
    "    total += weights[i, 0] * hl_out[:, i]\n",
    "    axs[i, 0].contourf(grid[0], grid[1], hl_out[:,i].reshape(grid[0].shape))\n",
    "    axs[i, 0].set_title(f\"+ {weights[i, 0]:.3f} *\")\n",
    "    axs[i, 1].contourf(grid[0], grid[1], total.numpy().reshape(grid[0].shape))\n",
    "    axs[i, 1].set_title(\"=\")\n",
    "    axs[i, 0].set_axis_off()\n",
    "    axs[i, 1].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f85b8-d722-4c60-a0f0-feb0b3cd20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f11dc5-6756-4deb-843f-bdf131e76794",
   "metadata": {},
   "source": [
    "This gives a nice idea about how a NN composes it's output by combining the outputs of the previous layer. A nice visualization of this can be seen at https://playground.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}